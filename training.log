(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) ✗ python trainingstarcoder.py --max_steps 5000 --resume_from_checkpoint finetune_starcoder2/checkpoint-1000

^[[A^[[Atrainable params: 151369728 || all params: 1591200768 || trainable%: 9.5129245186488
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Training...
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: WARNING Invalid choice
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.1
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: WARNING URL not available in offline run
{'loss': 2.9314, 'grad_norm': 0.3426077663898468, 'learning_rate': 2e-05, 'epoch': 0.004}
{'loss': 2.8247, 'grad_norm': 0.5724144577980042, 'learning_rate': 4e-05, 'epoch': 0.008}
{'loss': 2.5122, 'grad_norm': 0.7688854336738586, 'learning_rate': 6e-05, 'epoch': 0.012}
{'loss': 2.7368, 'grad_norm': 0.8625675439834595, 'learning_rate': 8e-05, 'epoch': 0.016}
{'loss': 2.3019, 'grad_norm': 1.1354831457138062, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 2.3206, 'grad_norm': 3.378488540649414, 'learning_rate': 0.00012, 'epoch': 0.024}
{'loss': 2.0744, 'grad_norm': 1.3570775985717773, 'learning_rate': 0.00014, 'epoch': 0.028}
{'loss': 1.8436, 'grad_norm': nan, 'learning_rate': 0.00015800000000000002, 'epoch': 0.032}
{'loss': 2.3957, 'grad_norm': 4.027080535888672, 'learning_rate': 0.00017800000000000002, 'epoch': 0.036}
{'loss': 2.1942, 'grad_norm': 1.2452301979064941, 'learning_rate': 0.00019800000000000002, 'epoch': 0.04}
{'loss': 1.8502, 'grad_norm': 4.084047317504883, 'learning_rate': 0.0001999983352025441, 'epoch': 0.044}
{'loss': 1.8054, 'grad_norm': 2.900085210800171, 'learning_rate': 0.00019999258041830607, 'epoch': 0.048}
{'loss': 1.9059, 'grad_norm': 3.3914549350738525, 'learning_rate': 0.0001999827153307312, 'epoch': 0.052}
{'loss': 1.496, 'grad_norm': 2.0497732162475586, 'learning_rate': 0.00019996874034533462, 'epoch': 0.056}
{'loss': 2.0725, 'grad_norm': 1.8106114864349365, 'learning_rate': 0.00019995065603657316, 'epoch': 0.06}
{'loss': 1.9731, 'grad_norm': 1.7514961957931519, 'learning_rate': 0.000199928463147822, 'epoch': 0.064}
{'loss': 1.6582, 'grad_norm': 3.570523262023926, 'learning_rate': 0.00019990216259134386, 'epoch': 0.068}
{'loss': 2.0631, 'grad_norm': 1.4778298139572144, 'learning_rate': 0.0001998717554482516, 'epoch': 0.072}
{'loss': 1.6277, 'grad_norm': 4.078042507171631, 'learning_rate': 0.00019983724296846375, 'epoch': 0.076}
{'loss': 1.8355, 'grad_norm': 1.9864320755004883, 'learning_rate': 0.00019979862657065324, 'epoch': 0.08}
{'loss': 1.9431, 'grad_norm': 2.8769729137420654, 'learning_rate': 0.00019975590784218888, 'epoch': 0.084}
{'loss': 1.7316, 'grad_norm': 4.924728870391846, 'learning_rate': 0.00019970908853907026, 'epoch': 0.088}
{'loss': 1.7061, 'grad_norm': 1.9807220697402954, 'learning_rate': 0.00019965817058585563, 'epoch': 0.092}
{'loss': 1.7857, 'grad_norm': 3.0840351581573486, 'learning_rate': 0.00019960315607558255, 'epoch': 0.096}
{'loss': 1.6178, 'grad_norm': 3.5300915241241455, 'learning_rate': 0.0001995440472696821, 'epoch': 0.1}
{'loss': 1.4387, 'grad_norm': 1.3585745096206665, 'learning_rate': 0.00019948084659788574, 'epoch': 0.104}
{'loss': 1.8342, 'grad_norm': 6.052093029022217, 'learning_rate': 0.00019941355665812558, 'epoch': 0.108}
{'loss': 1.9202, 'grad_norm': 1.362105131149292, 'learning_rate': 0.00019934218021642748, 'epoch': 0.112}
{'loss': 1.633, 'grad_norm': 3.1927781105041504, 'learning_rate': 0.00019926672020679736, 'epoch': 0.116}
{'loss': 2.0792, 'grad_norm': 1.9030426740646362, 'learning_rate': 0.00019918717973110072, 'epoch': 0.12}
{'loss': 1.5047, 'grad_norm': 1.5642791986465454, 'learning_rate': 0.0001991035620589349, 'epoch': 0.124}
{'loss': 1.6355, 'grad_norm': 2.054460048675537, 'learning_rate': 0.000199015870627495, 'epoch': 0.128}
{'loss': 1.5409, 'grad_norm': 2.1825244426727295, 'learning_rate': 0.00019892410904143222, 'epoch': 0.132}
{'loss': 1.8481, 'grad_norm': 1.5228831768035889, 'learning_rate': 0.00019882828107270598, 'epoch': 0.136}
{'loss': 1.7745, 'grad_norm': 5.09263801574707, 'learning_rate': 0.00019872839066042874, 'epoch': 0.14}
{'loss': 1.6073, 'grad_norm': 3.2239630222320557, 'learning_rate': 0.00019862444191070408, 'epoch': 0.144}
{'loss': 1.5039, 'grad_norm': 1.4044709205627441, 'learning_rate': 0.00019851643909645804, 'epoch': 0.148}
{'loss': 1.6572, 'grad_norm': 3.51741623878479, 'learning_rate': 0.0001984043866572632, 'epoch': 0.152}
{'loss': 1.6853, 'grad_norm': 0.8841949105262756, 'learning_rate': 0.0001982882891991565, 'epoch': 0.156}
{'loss': 1.5475, 'grad_norm': 1.8144136667251587, 'learning_rate': 0.00019816815149444977, 'epoch': 0.16}
{'loss': 1.5933, 'grad_norm': 0.8289576768875122, 'learning_rate': 0.00019804397848153342, 'epoch': 0.164}
{'loss': 1.3729, 'grad_norm': 0.7508878111839294, 'learning_rate': 0.0001979157752646737, 'epoch': 0.168}
{'loss': 1.536, 'grad_norm': 2.1276731491088867, 'learning_rate': 0.0001977835471138027, 'epoch': 0.172}
{'loss': 1.7872, 'grad_norm': 1.8165258169174194, 'learning_rate': 0.00019764729946430184, 'epoch': 0.176}
{'loss': 1.4654, 'grad_norm': 2.383781909942627, 'learning_rate': 0.00019750703791677828, 'epoch': 0.18}
{'loss': 1.4823, 'grad_norm': 5.184279918670654, 'learning_rate': 0.0001973627682368349, 'epoch': 0.184}
{'loss': 1.64, 'grad_norm': 2.337721109390259, 'learning_rate': 0.0001972144963548332, 'epoch': 0.188}
{'loss': 1.4945, 'grad_norm': 1.0584999322891235, 'learning_rate': 0.00019706222836564952, 'epoch': 0.192}
{'loss': 1.5313, 'grad_norm': 2.0738327503204346, 'learning_rate': 0.00019690597052842446, 'epoch': 0.196}
{'loss': 1.6322, 'grad_norm': 2.115068197250366, 'learning_rate': 0.00019674572926630567, 'epoch': 0.2}
{'loss': 1.5415, 'grad_norm': 1.3315216302871704, 'learning_rate': 0.00019658151116618385, 'epoch': 0.204}
{'loss': 1.435, 'grad_norm': 1.3839948177337646, 'learning_rate': 0.0001964133229784219, 'epoch': 0.208}
{'loss': 1.5693, 'grad_norm': 5.277719974517822, 'learning_rate': 0.00019624117161657752, 'epoch': 0.212}
{'loss': 1.7607, 'grad_norm': 1.7046406269073486, 'learning_rate': 0.00019606506415711881, 'epoch': 0.216}
{'loss': 1.4566, 'grad_norm': 2.4589056968688965, 'learning_rate': 0.00019588500783913375, 'epoch': 0.22}
{'loss': 1.4359, 'grad_norm': 3.3946352005004883, 'learning_rate': 0.00019570101006403226, 'epoch': 0.224}
{'loss': 1.4338, 'grad_norm': 1.8414881229400635, 'learning_rate': 0.0001955130783952423, 'epoch': 0.228}
{'loss': 1.2068, 'grad_norm': 0.9358095526695251, 'learning_rate': 0.00019532122055789864, 'epoch': 0.232}
{'loss': 1.3874, 'grad_norm': 1.4463573694229126, 'learning_rate': 0.00019512544443852557, 'epoch': 0.236}
{'loss': 1.7293, 'grad_norm': 3.279111385345459, 'learning_rate': 0.00019492575808471247, 'epoch': 0.24}
{'loss': 1.3392, 'grad_norm': 1.105846881866455, 'learning_rate': 0.00019472216970478328, 'epoch': 0.244}
{'loss': 1.3584, 'grad_norm': 1.1813045740127563, 'learning_rate': 0.0001945146876674589, 'epoch': 0.248}
{'loss': 1.6823, 'grad_norm': 4.897843837738037, 'learning_rate': 0.00019430332050151324, 'epoch': 0.252}
{'loss': 1.509, 'grad_norm': 2.2838189601898193, 'learning_rate': 0.00019408807689542257, 'epoch': 0.256}
{'loss': 1.3766, 'grad_norm': 2.1784093379974365, 'learning_rate': 0.00019386896569700853, 'epoch': 0.26}
{'loss': 1.4166, 'grad_norm': 0.8633360266685486, 'learning_rate': 0.00019364599591307425, 'epoch': 0.264}
{'loss': 1.2089, 'grad_norm': 0.9131094217300415, 'learning_rate': 0.0001934191767090343, 'epoch': 0.268}
{'loss': 1.7495, 'grad_norm': 1.5715149641036987, 'learning_rate': 0.0001931885174085377, 'epoch': 0.272}
{'loss': 1.7577, 'grad_norm': 0.6264223456382751, 'learning_rate': 0.00019295402749308497, 'epoch': 0.276}
{'loss': 1.5309, 'grad_norm': 2.2082910537719727, 'learning_rate': 0.00019271571660163793, 'epoch': 0.28}
{'loss': 1.5806, 'grad_norm': 3.826303005218506, 'learning_rate': 0.00019247359453022407, 'epoch': 0.284}
{'loss': 1.4271, 'grad_norm': 0.7209219336509705, 'learning_rate': 0.00019222767123153336, 'epoch': 0.288}
{'loss': 1.452, 'grad_norm': 2.2184195518493652, 'learning_rate': 0.00019197795681450937, 'epoch': 0.292}
{'loss': 1.433, 'grad_norm': 1.07893705368042, 'learning_rate': 0.0001917244615439338, 'epoch': 0.296}
{'loss': 1.3766, 'grad_norm': 1.4187618494033813, 'learning_rate': 0.00019146719584000428, 'epoch': 0.3}
{'loss': 1.6586, 'grad_norm': 2.4059064388275146, 'learning_rate': 0.0001912061702779063, 'epoch': 0.304}
{'loss': 1.5635, 'grad_norm': 1.0006306171417236, 'learning_rate': 0.0001909413955873783, 'epoch': 0.308}
{'loss': 1.5299, 'grad_norm': 1.299811601638794, 'learning_rate': 0.00019067288265227082, 'epoch': 0.312}
{'loss': 1.2976, 'grad_norm': 0.9722449779510498, 'learning_rate': 0.00019040064251009886, 'epoch': 0.316}
{'loss': 1.7872, 'grad_norm': 1.4584935903549194, 'learning_rate': 0.00019012468635158845, 'epoch': 0.32}
{'loss': 1.584, 'grad_norm': 2.7648754119873047, 'learning_rate': 0.00018984502552021635, 'epoch': 0.324}
{'loss': 1.5812, 'grad_norm': 1.7412203550338745, 'learning_rate': 0.000189561671511744, 'epoch': 0.328}
{'loss': 1.6313, 'grad_norm': 1.3458647727966309, 'learning_rate': 0.0001892746359737449, 'epoch': 0.332}
{'loss': 1.5319, 'grad_norm': 1.460942268371582, 'learning_rate': 0.00018898393070512572, 'epoch': 0.336}
{'loss': 1.52, 'grad_norm': 0.9537712335586548, 'learning_rate': 0.0001886895676556415, 'epoch': 0.34}
{'loss': 1.5534, 'grad_norm': 2.171046257019043, 'learning_rate': 0.00018839155892540424, 'epoch': 0.344}
{'loss': 1.3827, 'grad_norm': 0.891879141330719, 'learning_rate': 0.0001880899167643856, 'epoch': 0.348}
{'loss': 1.618, 'grad_norm': 3.126929521560669, 'learning_rate': 0.0001877846535719134, 'epoch': 0.352}
{'loss': 1.6419, 'grad_norm': 2.0574615001678467, 'learning_rate': 0.0001874757818961618, 'epoch': 0.356}
{'loss': 1.3158, 'grad_norm': 1.8916999101638794, 'learning_rate': 0.00018716331443363563, 'epoch': 0.36}
{'loss': 1.276, 'grad_norm': 0.4915868639945984, 'learning_rate': 0.0001868472640286485, 'epoch': 0.364}
{'loss': 1.6373, 'grad_norm': 7.387997150421143, 'learning_rate': 0.00018652764367279461, 'epoch': 0.368}
{'loss': 1.6126, 'grad_norm': 2.565603733062744, 'learning_rate': 0.0001862044665044149, 'epoch': 0.372}
{'loss': 1.4384, 'grad_norm': 1.8439942598342896, 'learning_rate': 0.00018587774580805703, 'epoch': 0.376}
{'loss': 1.3861, 'grad_norm': 2.3556010723114014, 'learning_rate': 0.0001855474950139291, 'epoch': 0.38}
{'loss': 1.4213, 'grad_norm': 3.6868979930877686, 'learning_rate': 0.00018521372769734774, 'epoch': 0.384}
{'loss': 1.486, 'grad_norm': 2.39813232421875, 'learning_rate': 0.00018487645757818015, 'epoch': 0.388}
{'loss': 1.4516, 'grad_norm': 0.8430299758911133, 'learning_rate': 0.0001845356985202798, 'epoch': 0.392}
{'loss': 1.4761, 'grad_norm': 1.346730351448059, 'learning_rate': 0.00018419146453091701, 'epoch': 0.396}
{'loss': 1.4382, 'grad_norm': 0.6636066436767578, 'learning_rate': 0.00018384376976020276, 'epoch': 0.4}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.4557, 'grad_norm': 1.3363045454025269, 'learning_rate': 0.00018349262850050722, 'epoch': 0.404}
{'loss': 1.3329, 'grad_norm': 3.7786483764648438, 'learning_rate': 0.00018313805518587232, 'epoch': 0.408}
{'loss': 1.4729, 'grad_norm': 0.9776219129562378, 'learning_rate': 0.0001827800643914182, 'epoch': 0.412}
{'loss': 1.222, 'grad_norm': 2.0280978679656982, 'learning_rate': 0.0001824186708327443, 'epoch': 0.416}
{'loss': 1.3145, 'grad_norm': 0.7734864354133606, 'learning_rate': 0.0001820538893653243, 'epoch': 0.42}
{'loss': 1.188, 'grad_norm': 2.2938854694366455, 'learning_rate': 0.00018168573498389564, 'epoch': 0.424}
{'loss': 1.5075, 'grad_norm': 1.8473442792892456, 'learning_rate': 0.00018131422282184286, 'epoch': 0.428}
{'loss': 1.4227, 'grad_norm': 3.4017581939697266, 'learning_rate': 0.00018093936815057594, 'epoch': 0.432}
{'loss': 1.4217, 'grad_norm': 1.225844383239746, 'learning_rate': 0.00018056118637890217, 'epoch': 0.436}
{'loss': 1.3632, 'grad_norm': 0.6700549125671387, 'learning_rate': 0.00018017969305239297, 'epoch': 0.44}
{'loss': 1.4224, 'grad_norm': 2.929745674133301, 'learning_rate': 0.00017979490385274473, 'epoch': 0.444}
{'loss': 1.2934, 'grad_norm': 2.18133544921875, 'learning_rate': 0.0001794068345971344, 'epoch': 0.448}
{'loss': 1.4827, 'grad_norm': 1.2220101356506348, 'learning_rate': 0.00017901550123756906, 'epoch': 0.452}
{'loss': 1.4306, 'grad_norm': 1.3870185613632202, 'learning_rate': 0.0001786209198602304, 'epoch': 0.456}
{'loss': 1.4062, 'grad_norm': 2.0105984210968018, 'learning_rate': 0.00017822310668481333, 'epoch': 0.46}
{'loss': 1.2138, 'grad_norm': 0.8706759810447693, 'learning_rate': 0.00017782207806385945, 'epoch': 0.464}
{'loss': 1.4584, 'grad_norm': 1.0097064971923828, 'learning_rate': 0.00017741785048208458, 'epoch': 0.468}
{'loss': 1.2945, 'grad_norm': 0.8852534294128418, 'learning_rate': 0.00017701044055570136, 'epoch': 0.472}
{'loss': 1.61, 'grad_norm': 2.1827220916748047, 'learning_rate': 0.00017659986503173615, 'epoch': 0.476}
{'loss': 1.4513, 'grad_norm': 2.3580217361450195, 'learning_rate': 0.00017618614078734069, 'epoch': 0.48}
{'loss': 1.4208, 'grad_norm': 6.963593006134033, 'learning_rate': 0.00017576928482909812, 'epoch': 0.484}
{'loss': 1.3828, 'grad_norm': 3.9398324489593506, 'learning_rate': 0.00017534931429232423, 'epoch': 0.488}
{'loss': 1.1996, 'grad_norm': 1.9872972965240479, 'learning_rate': 0.00017492624644036285, 'epoch': 0.492}
{'loss': 1.5255, 'grad_norm': 1.0024183988571167, 'learning_rate': 0.00017450009866387634, 'epoch': 0.496}
{'loss': 1.201, 'grad_norm': 0.8361837267875671, 'learning_rate': 0.00017407088848013071, 'epoch': 0.5}
{'loss': 1.4786, 'grad_norm': 3.1053545475006104, 'learning_rate': 0.00017363863353227546, 'epoch': 0.504}
{'loss': 1.4706, 'grad_norm': 1.968544363975525, 'learning_rate': 0.00017320335158861855, 'epoch': 0.508}
{'loss': 1.1014, 'grad_norm': 1.001348614692688, 'learning_rate': 0.0001727650605418957, 'epoch': 0.512}
{'loss': 1.7147, 'grad_norm': 1.1906787157058716, 'learning_rate': 0.00017232377840853522, 'epoch': 0.516}
{'loss': 1.5824, 'grad_norm': 1.4435288906097412, 'learning_rate': 0.00017187952332791727, 'epoch': 0.52}
{'loss': 1.6831, 'grad_norm': 1.4517748355865479, 'learning_rate': 0.0001714323135616281, 'epoch': 0.524}
{'loss': 1.6844, 'grad_norm': 3.6160871982574463, 'learning_rate': 0.00017098216749270967, 'epoch': 0.528}
{'loss': 1.4034, 'grad_norm': 1.0457487106323242, 'learning_rate': 0.00017052910362490376, 'epoch': 0.532}
{'loss': 1.3174, 'grad_norm': 1.9017677307128906, 'learning_rate': 0.0001700731405818914, 'epoch': 0.536}
{'loss': 1.5534, 'grad_norm': 0.7176085710525513, 'learning_rate': 0.00016961429710652746, 'epoch': 0.54}
{'loss': 1.3121, 'grad_norm': 1.1929118633270264, 'learning_rate': 0.00016915259206007002, 'epoch': 0.544}
{'loss': 1.3575, 'grad_norm': 0.5831860303878784, 'learning_rate': 0.00016868804442140517, 'epoch': 0.548}
{'loss': 1.3894, 'grad_norm': 1.9680525064468384, 'learning_rate': 0.00016822067328626684, 'epoch': 0.552}
{'loss': 1.4327, 'grad_norm': 1.0704572200775146, 'learning_rate': 0.00016775049786645177, 'epoch': 0.556}
{'loss': 1.1943, 'grad_norm': 1.9090466499328613, 'learning_rate': 0.00016727753748903, 'epoch': 0.56}
{'loss': 1.476, 'grad_norm': 0.902545154094696, 'learning_rate': 0.00016680181159555013, 'epoch': 0.564}
{'loss': 1.4098, 'grad_norm': 2.748033285140991, 'learning_rate': 0.0001663233397412404, 'epoch': 0.568}
{'loss': 1.5587, 'grad_norm': 2.057060480117798, 'learning_rate': 0.00016584214159420463, 'epoch': 0.572}
{'loss': 1.2305, 'grad_norm': 1.0173211097717285, 'learning_rate': 0.00016535823693461394, 'epoch': 0.576}
{'loss': 1.5984, 'grad_norm': 0.8315314650535583, 'learning_rate': 0.0001648716456538936, 'epoch': 0.58}
{'loss': 1.4016, 'grad_norm': 2.0302441120147705, 'learning_rate': 0.00016438238775390526, 'epoch': 0.584}
{'loss': 1.5623, 'grad_norm': 1.3281930685043335, 'learning_rate': 0.00016389048334612493, 'epoch': 0.588}
{'loss': 1.2105, 'grad_norm': 2.5813257694244385, 'learning_rate': 0.0001633959526508162, 'epoch': 0.592}
{'loss': 1.293, 'grad_norm': 2.606473684310913, 'learning_rate': 0.00016289881599619889, 'epoch': 0.596}
{'loss': 1.4706, 'grad_norm': 2.6753194332122803, 'learning_rate': 0.0001623990938176138, 'epoch': 0.6}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.3814, 'grad_norm': 1.7082539796829224, 'learning_rate': 0.00016189680665668242, 'epoch': 0.604}
{'loss': 1.0857, 'grad_norm': 2.3458569049835205, 'learning_rate': 0.0001613919751604626, 'epoch': 0.608}
{'loss': 1.4851, 'grad_norm': 1.2993898391723633, 'learning_rate': 0.00016088462008059982, 'epoch': 0.612}
{'loss': 1.617, 'grad_norm': 2.113675832748413, 'learning_rate': 0.00016037476227247427, 'epoch': 0.616}
{'loss': 1.3041, 'grad_norm': 1.4251325130462646, 'learning_rate': 0.00015986242269434354, 'epoch': 0.62}
{'loss': 1.3016, 'grad_norm': 1.1263073682785034, 'learning_rate': 0.00015934762240648085, 'epoch': 0.624}
{'loss': 1.3004, 'grad_norm': 1.232686996459961, 'learning_rate': 0.00015883038257030976, 'epoch': 0.628}
{'loss': 1.3501, 'grad_norm': 3.882359743118286, 'learning_rate': 0.0001583107244475341, 'epoch': 0.632}
{'loss': 1.3114, 'grad_norm': 1.6208432912826538, 'learning_rate': 0.0001577886693992639, 'epoch': 0.636}
{'loss': 1.1897, 'grad_norm': 1.7327466011047363, 'learning_rate': 0.00015726423888513737, 'epoch': 0.64}
{'loss': 1.291, 'grad_norm': 3.1010141372680664, 'learning_rate': 0.000156737454462439, 'epoch': 0.644}
{'loss': 1.4081, 'grad_norm': 3.1873507499694824, 'learning_rate': 0.00015620833778521307, 'epoch': 0.648}
{'loss': 1.4853, 'grad_norm': 1.6985790729522705, 'learning_rate': 0.00015567691060337378, 'epoch': 0.652}
{'loss': 1.2898, 'grad_norm': 1.6038178205490112, 'learning_rate': 0.00015514319476181117, 'epoch': 0.656}
{'loss': 1.2127, 'grad_norm': 0.5976592302322388, 'learning_rate': 0.000154607212199493, 'epoch': 0.66}
{'loss': 1.5558, 'grad_norm': 3.829710006713867, 'learning_rate': 0.00015406898494856313, 'epoch': 0.664}
{'loss': 1.5769, 'grad_norm': 1.7985687255859375, 'learning_rate': 0.00015352853513343572, 'epoch': 0.668}
{'loss': 1.5362, 'grad_norm': 1.4667543172836304, 'learning_rate': 0.00015298588496988596, 'epoch': 0.672}
{'loss': 1.4222, 'grad_norm': 2.4698691368103027, 'learning_rate': 0.0001524410567641366, 'epoch': 0.676}
{'loss': 1.5594, 'grad_norm': 1.7262383699417114, 'learning_rate': 0.0001518940729119412, 'epoch': 0.68}
{'loss': 1.2259, 'grad_norm': 1.224015235900879, 'learning_rate': 0.0001513449558976636, 'epoch': 0.684}
{'loss': 1.2538, 'grad_norm': 0.7717073559761047, 'learning_rate': 0.00015079372829335347, 'epoch': 0.688}
{'loss': 1.5336, 'grad_norm': 0.5411852598190308, 'learning_rate': 0.00015024041275781862, 'epoch': 0.692}
{'loss': 1.2816, 'grad_norm': 1.2140952348709106, 'learning_rate': 0.00014968503203569356, 'epoch': 0.696}
{'loss': 1.3269, 'grad_norm': 1.186733365058899, 'learning_rate': 0.00014912760895650445, 'epoch': 0.7}
{'loss': 1.463, 'grad_norm': 0.9411900043487549, 'learning_rate': 0.00014856816643373083, 'epoch': 0.704}
{'loss': 1.3961, 'grad_norm': 1.1804002523422241, 'learning_rate': 0.00014800672746386365, 'epoch': 0.708}
{'loss': 1.3045, 'grad_norm': 1.173919677734375, 'learning_rate': 0.00014744331512545988, 'epoch': 0.712}
{'loss': 1.4382, 'grad_norm': 0.8345919251441956, 'learning_rate': 0.00014687795257819407, 'epoch': 0.716}
{'loss': 1.351, 'grad_norm': 3.9308226108551025, 'learning_rate': 0.00014631066306190614, 'epoch': 0.72}
{'loss': 1.3262, 'grad_norm': 0.9781283736228943, 'learning_rate': 0.0001457414698956462, 'epoch': 0.724}
{'loss': 1.2942, 'grad_norm': 2.5757157802581787, 'learning_rate': 0.00014517039647671593, 'epoch': 0.728}
{'loss': 1.4895, 'grad_norm': 1.083965539932251, 'learning_rate': 0.00014459746627970685, 'epoch': 0.732}
{'loss': 1.3182, 'grad_norm': 0.7997762560844421, 'learning_rate': 0.00014402270285553535, 'epoch': 0.736}
{'loss': 1.2634, 'grad_norm': 2.4980828762054443, 'learning_rate': 0.00014344612983047459, 'epoch': 0.74}
{'loss': 1.2662, 'grad_norm': 1.829947829246521, 'learning_rate': 0.00014286777090518333, 'epoch': 0.744}
{'loss': 1.3991, 'grad_norm': 0.8795732855796814, 'learning_rate': 0.00014228764985373175, 'epoch': 0.748}
{'loss': 1.403, 'grad_norm': 0.5093012452125549, 'learning_rate': 0.00014170579052262406, 'epoch': 0.752}
{'loss': 1.3026, 'grad_norm': 1.46115243434906, 'learning_rate': 0.00014112221682981843, 'epoch': 0.756}
{'loss': 1.1484, 'grad_norm': 1.522019624710083, 'learning_rate': 0.0001405369527637436, 'epoch': 0.76}
{'loss': 1.4105, 'grad_norm': 1.4139145612716675, 'learning_rate': 0.00013995002238231308, 'epoch': 0.764}
{'loss': 1.4448, 'grad_norm': 11.04308032989502, 'learning_rate': 0.00013936144981193592, 'epoch': 0.768}
{'loss': 1.3987, 'grad_norm': 1.4180097579956055, 'learning_rate': 0.00013877125924652525, 'epoch': 0.772}
{'loss': 1.2018, 'grad_norm': 0.8894153833389282, 'learning_rate': 0.0001381794749465036, 'epoch': 0.776}
{'loss': 1.2239, 'grad_norm': 0.8420198559761047, 'learning_rate': 0.00013758612123780566, 'epoch': 0.78}
{'loss': 1.4111, 'grad_norm': 0.6698972582817078, 'learning_rate': 0.00013699122251087842, 'epoch': 0.784}
{'loss': 1.3668, 'grad_norm': 1.2630467414855957, 'learning_rate': 0.00013639480321967845, 'epoch': 0.788}
{'loss': 1.3914, 'grad_norm': 0.6840726137161255, 'learning_rate': 0.00013579688788066684, 'epoch': 0.792}
{'loss': 1.421, 'grad_norm': 3.763947010040283, 'learning_rate': 0.00013519750107180125, 'epoch': 0.796}
{'loss': 1.2342, 'grad_norm': 2.331861972808838, 'learning_rate': 0.00013459666743152577, 'epoch': 0.8}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.2921, 'grad_norm': 0.6564565300941467, 'learning_rate': 0.00013399441165775808, 'epoch': 0.804}
{'loss': 1.4038, 'grad_norm': 1.9154822826385498, 'learning_rate': 0.00013339075850687414, 'epoch': 0.808}
{'loss': 1.5269, 'grad_norm': 1.1938068866729736, 'learning_rate': 0.00013278573279269063, 'epoch': 0.812}
{'loss': 1.5001, 'grad_norm': 1.3147897720336914, 'learning_rate': 0.00013217935938544497, 'epoch': 0.816}
{'loss': 1.4229, 'grad_norm': 0.6418153643608093, 'learning_rate': 0.00013157166321077287, 'epoch': 0.82}
{'loss': 1.2517, 'grad_norm': 1.7514050006866455, 'learning_rate': 0.00013096266924868393, 'epoch': 0.824}
{'loss': 1.4781, 'grad_norm': 4.415118217468262, 'learning_rate': 0.00013035240253253468, 'epoch': 0.828}
{'loss': 1.3421, 'grad_norm': 1.260333776473999, 'learning_rate': 0.00012974088814799952, 'epoch': 0.832}
{'loss': 1.5266, 'grad_norm': 5.346252918243408, 'learning_rate': 0.00012912815123203972, 'epoch': 0.836}
{'loss': 1.2073, 'grad_norm': 0.5717644095420837, 'learning_rate': 0.00012851421697186992, 'epoch': 0.84}
{'loss': 1.3588, 'grad_norm': 2.445136070251465, 'learning_rate': 0.00012789911060392294, 'epoch': 0.844}
{'loss': 1.2628, 'grad_norm': 0.93324875831604, 'learning_rate': 0.00012728285741281237, 'epoch': 0.848}
{'loss': 1.5105, 'grad_norm': 2.03676700592041, 'learning_rate': 0.00012666548273029322, 'epoch': 0.852}
{'loss': 1.4108, 'grad_norm': 0.3933759927749634, 'learning_rate': 0.0001260470119342206, 'epoch': 0.856}
{'loss': 1.3586, 'grad_norm': 1.9751585721969604, 'learning_rate': 0.0001254274704475065, 'epoch': 0.86}
{'loss': 1.4167, 'grad_norm': 1.4171698093414307, 'learning_rate': 0.00012480688373707493, 'epoch': 0.864}
{'loss': 1.2637, 'grad_norm': 0.5503426194190979, 'learning_rate': 0.0001241852773128148, 'epoch': 0.868}
{'loss': 1.2271, 'grad_norm': 0.9212702512741089, 'learning_rate': 0.0001235626767265316, 'epoch': 0.872}
{'loss': 1.1633, 'grad_norm': 1.01658296585083, 'learning_rate': 0.00012293910757089688, 'epoch': 0.876}
{'loss': 1.4946, 'grad_norm': 1.3394322395324707, 'learning_rate': 0.00012231459547839627, 'epoch': 0.88}
{'loss': 1.3649, 'grad_norm': 0.6964792013168335, 'learning_rate': 0.00012168916612027581, 'epoch': 0.884}
{'loss': 1.118, 'grad_norm': 0.6600357294082642, 'learning_rate': 0.0001210628452054868, 'epoch': 0.888}
{'loss': 1.1453, 'grad_norm': 1.0443928241729736, 'learning_rate': 0.00012043565847962895, 'epoch': 0.892}
{'loss': 1.1928, 'grad_norm': 0.9108535647392273, 'learning_rate': 0.00011980763172389197, 'epoch': 0.896}
{'loss': 1.4911, 'grad_norm': 1.5070407390594482, 'learning_rate': 0.000119178790753996, 'epoch': 0.9}
{'loss': 1.483, 'grad_norm': 1.0056109428405762, 'learning_rate': 0.00011854916141913035, 'epoch': 0.904}
{'loss': 1.4033, 'grad_norm': 1.0658925771713257, 'learning_rate': 0.00011791876960089086, 'epoch': 0.908}
{'loss': 1.1826, 'grad_norm': 1.2630685567855835, 'learning_rate': 0.00011728764121221605, 'epoch': 0.912}
{'loss': 1.4103, 'grad_norm': 0.7019053101539612, 'learning_rate': 0.00011665580219632208, 'epoch': 0.916}
{'loss': 1.4854, 'grad_norm': 1.3955696821212769, 'learning_rate': 0.00011602327852563617, 'epoch': 0.92}
{'loss': 1.319, 'grad_norm': 2.633559465408325, 'learning_rate': 0.00011539009620072893, 'epoch': 0.924}
{'loss': 1.0899, 'grad_norm': 0.8198688626289368, 'learning_rate': 0.00011475628124924577, 'epoch': 0.928}
{'loss': 1.6464, 'grad_norm': 1.3325400352478027, 'learning_rate': 0.00011412185972483684, 'epoch': 0.932}
{'loss': 1.2232, 'grad_norm': 1.4340524673461914, 'learning_rate': 0.0001134868577060862, 'epoch': 0.936}
{'loss': 1.3255, 'grad_norm': 0.8404746055603027, 'learning_rate': 0.00011285130129543964, 'epoch': 0.94}
{'loss': 1.2992, 'grad_norm': 1.4326735734939575, 'learning_rate': 0.00011221521661813197, 'epoch': 0.944}
{'loss': 1.2611, 'grad_norm': 0.8859516978263855, 'learning_rate': 0.0001115786298211128, 'epoch': 0.948}
{'loss': 1.6757, 'grad_norm': 2.1931865215301514, 'learning_rate': 0.0001109415670719721, 'epoch': 0.952}
{'loss': 1.2104, 'grad_norm': 1.4202946424484253, 'learning_rate': 0.00011030405455786425, 'epoch': 0.956}
{'loss': 1.3391, 'grad_norm': 1.156833291053772, 'learning_rate': 0.00010966611848443176, 'epoch': 0.96}
{'loss': 1.2773, 'grad_norm': 0.5887752771377563, 'learning_rate': 0.00010902778507472799, 'epoch': 0.964}
{'loss': 1.2979, 'grad_norm': 3.3123838901519775, 'learning_rate': 0.00010838908056813919, 'epoch': 0.968}
{'loss': 1.1257, 'grad_norm': 3.0220324993133545, 'learning_rate': 0.00010775003121930602, 'epoch': 0.972}
{'loss': 1.1116, 'grad_norm': 0.9887374043464661, 'learning_rate': 0.00010711066329704423, 'epoch': 0.976}
{'loss': 1.2953, 'grad_norm': 1.1024693250656128, 'learning_rate': 0.00010647100308326484, 'epoch': 0.98}
{'loss': 1.268, 'grad_norm': 0.8385027647018433, 'learning_rate': 0.00010583107687189388, 'epoch': 0.984}
{'loss': 1.2998, 'grad_norm': 1.647973895072937, 'learning_rate': 0.0001051909109677915, 'epoch': 0.988}
{'loss': 1.4289, 'grad_norm': 3.389186382293701, 'learning_rate': 0.00010455053168567064, 'epoch': 0.992}
{'loss': 1.0462, 'grad_norm': 0.9220070242881775, 'learning_rate': 0.00010390996534901538, 'epoch': 0.996}
{'loss': 1.4156, 'grad_norm': 0.7397074699401855, 'learning_rate': 0.00010326923828899894, 'epoch': 1.0}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.4887, 'grad_norm': 0.9639027118682861, 'learning_rate': 0.00010262837684340112, 'epoch': 1.004}
{'loss': 1.2554, 'grad_norm': 2.182206630706787, 'learning_rate': 0.00010198740735552596, 'epoch': 1.008}
{'loss': 1.2221, 'grad_norm': 0.9603483080863953, 'learning_rate': 0.00010134635617311853, 'epoch': 1.012}
{'loss': 1.447, 'grad_norm': 3.4250192642211914, 'learning_rate': 0.00010070524964728218, 'epoch': 1.016}
{'loss': 1.05, 'grad_norm': 0.738854706287384, 'learning_rate': 0.00010006411413139507, 'epoch': 1.02}
{'loss': 1.042, 'grad_norm': 1.0492695569992065, 'learning_rate': 9.942297598002714e-05, 'epoch': 1.024}
{'loss': 1.4666, 'grad_norm': 2.492426872253418, 'learning_rate': 9.878186154785662e-05, 'epoch': 1.028}
{'loss': 1.2897, 'grad_norm': 3.1649575233459473, 'learning_rate': 9.814079718858677e-05, 'epoch': 1.032}
{'loss': 1.2416, 'grad_norm': 0.8872597813606262, 'learning_rate': 9.749980925386247e-05, 'epoch': 1.036}
{'loss': 1.231, 'grad_norm': 0.9051250219345093, 'learning_rate': 9.685892409218717e-05, 'epoch': 1.04}
{'loss': 1.1985, 'grad_norm': 0.8837795853614807, 'learning_rate': 9.62181680478397e-05, 'epoch': 1.044}
{'loss': 1.1849, 'grad_norm': 2.122621774673462, 'learning_rate': 9.557756745979138e-05, 'epoch': 1.048}
{'loss': 1.2959, 'grad_norm': 3.2190794944763184, 'learning_rate': 9.493714866062326e-05, 'epoch': 1.052}
{'loss': 1.5947, 'grad_norm': nan, 'learning_rate': 9.436094892873858e-05, 'epoch': 1.056}
{'loss': 1.1789, 'grad_norm': 0.6760316491127014, 'learning_rate': 9.372094804706867e-05, 'epoch': 1.06}
{'loss': 1.3573, 'grad_norm': 2.808229684829712, 'learning_rate': 9.308120527263117e-05, 'epoch': 1.064}
{'loss': 1.185, 'grad_norm': 1.1165239810943604, 'learning_rate': 9.24417469027459e-05, 'epoch': 1.068}
{'loss': 1.2705, 'grad_norm': 0.9368786215782166, 'learning_rate': 9.180259922304175e-05, 'epoch': 1.072}
{'loss': 1.3303, 'grad_norm': 2.1708762645721436, 'learning_rate': 9.11637885063765e-05, 'epoch': 1.076}
{'loss': 1.2228, 'grad_norm': 1.931166648864746, 'learning_rate': 9.052534101175672e-05, 'epoch': 1.08}
{'loss': 1.2875, 'grad_norm': 0.9844762086868286, 'learning_rate': 8.988728298325822e-05, 'epoch': 1.084}
{'loss': 1.1902, 'grad_norm': 0.5643959045410156, 'learning_rate': 8.924964064894753e-05, 'epoch': 1.088}
{'loss': 1.302, 'grad_norm': 1.7700437307357788, 'learning_rate': 8.861244021980344e-05, 'epoch': 1.092}
{'loss': 1.5425, 'grad_norm': 1.6792892217636108, 'learning_rate': 8.797570788863989e-05, 'epoch': 1.096}
{'loss': 1.3301, 'grad_norm': 1.2977845668792725, 'learning_rate': 8.733946982902911e-05, 'epoch': 1.1}
{'loss': 1.2757, 'grad_norm': 0.6045295596122742, 'learning_rate': 8.670375219422578e-05, 'epoch': 1.104}
{'loss': 1.337, 'grad_norm': 2.2370309829711914, 'learning_rate': 8.606858111609188e-05, 'epoch': 1.108}
{'loss': 1.3251, 'grad_norm': 0.7211747765541077, 'learning_rate': 8.543398270402266e-05, 'epoch': 1.112}
{'loss': 1.456, 'grad_norm': 1.2359495162963867, 'learning_rate': 8.479998304387329e-05, 'epoch': 1.116}
{'loss': 1.2096, 'grad_norm': 1.6622370481491089, 'learning_rate': 8.416660819688659e-05, 'epoch': 1.12}
{'loss': 1.3181, 'grad_norm': 2.0107059478759766, 'learning_rate': 8.353388419862178e-05, 'epoch': 1.124}
{'loss': 1.2869, 'grad_norm': 0.42989274859428406, 'learning_rate': 8.290183705788419e-05, 'epoch': 1.1280000000000001}
{'loss': 1.3076, 'grad_norm': 1.6494404077529907, 'learning_rate': 8.227049275565622e-05, 'epoch': 1.1320000000000001}
{'loss': 1.2356, 'grad_norm': 0.9478943347930908, 'learning_rate': 8.163987724402934e-05, 'epoch': 1.1360000000000001}
{'loss': 1.2827, 'grad_norm': 1.6503478288650513, 'learning_rate': 8.101001644513731e-05, 'epoch': 1.1400000000000001}
{'loss': 1.2678, 'grad_norm': 3.3344101905822754, 'learning_rate': 8.038093625009052e-05, 'epoch': 1.144}
{'loss': 1.2472, 'grad_norm': 2.387861490249634, 'learning_rate': 7.975266251791185e-05, 'epoch': 1.148}
{'loss': 1.226, 'grad_norm': 0.9998906850814819, 'learning_rate': 7.912522107447367e-05, 'epoch': 1.152}
{'loss': 1.2129, 'grad_norm': 1.958795428276062, 'learning_rate': 7.849863771143621e-05, 'epoch': 1.156}
{'loss': 1.2128, 'grad_norm': 1.3954683542251587, 'learning_rate': 7.787293818518738e-05, 'epoch': 1.16}
{'loss': 1.367, 'grad_norm': 1.722585678100586, 'learning_rate': 7.724814821578396e-05, 'epoch': 1.164}
{'loss': 1.1775, 'grad_norm': 6.562158584594727, 'learning_rate': 7.662429348589447e-05, 'epoch': 1.168}
{'loss': 1.3403, 'grad_norm': 1.7451854944229126, 'learning_rate': 7.600139963974341e-05, 'epoch': 1.172}
{'loss': 1.4728, 'grad_norm': 1.8296914100646973, 'learning_rate': 7.537949228205709e-05, 'epoch': 1.176}
{'loss': 1.3297, 'grad_norm': 1.8016961812973022, 'learning_rate': 7.47585969770111e-05, 'epoch': 1.18}
{'loss': 1.2714, 'grad_norm': 1.7298575639724731, 'learning_rate': 7.413873924717957e-05, 'epoch': 1.184}
{'loss': 1.2681, 'grad_norm': 3.7214252948760986, 'learning_rate': 7.351994457248595e-05, 'epoch': 1.188}
{'loss': 1.3549, 'grad_norm': 0.9401415586471558, 'learning_rate': 7.290223838915568e-05, 'epoch': 1.192}
{'loss': 1.5495, 'grad_norm': 2.476270914077759, 'learning_rate': 7.22856460886706e-05, 'epoch': 1.196}
{'loss': 1.6247, 'grad_norm': 1.5147844552993774, 'learning_rate': 7.167019301672509e-05, 'epoch': 1.2}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.2005, 'grad_norm': 0.7994626760482788, 'learning_rate': 7.105590447218438e-05, 'epoch': 1.204}
{'loss': 1.2821, 'grad_norm': 0.7776494026184082, 'learning_rate': 7.044280570604451e-05, 'epoch': 1.208}
{'loss': 1.0409, 'grad_norm': 1.1810953617095947, 'learning_rate': 6.983092192039456e-05, 'epoch': 1.212}
{'loss': 1.2533, 'grad_norm': 0.7224768996238708, 'learning_rate': 6.922027826738019e-05, 'epoch': 1.216}
{'loss': 1.3362, 'grad_norm': 2.068134069442749, 'learning_rate': 6.861089984817032e-05, 'epoch': 1.22}
{'loss': 1.3673, 'grad_norm': 1.5544548034667969, 'learning_rate': 6.800281171192501e-05, 'epoch': 1.224}
{'loss': 1.3353, 'grad_norm': 2.2130820751190186, 'learning_rate': 6.739603885476582e-05, 'epoch': 1.228}
{'loss': 1.5654, 'grad_norm': 1.981311559677124, 'learning_rate': 6.679060621874833e-05, 'epoch': 1.232}
{'loss': 1.2282, 'grad_norm': 1.4292455911636353, 'learning_rate': 6.618653869083688e-05, 'epoch': 1.236}
{'loss': 1.1098, 'grad_norm': 0.9447025656700134, 'learning_rate': 6.558386110188157e-05, 'epoch': 1.24}
{'loss': 1.33, 'grad_norm': 3.734339714050293, 'learning_rate': 6.498259822559758e-05, 'epoch': 1.244}
{'loss': 1.1973, 'grad_norm': 0.6856318712234497, 'learning_rate': 6.438277477754679e-05, 'epoch': 1.248}
{'loss': 1.1503, 'grad_norm': 0.4415625333786011, 'learning_rate': 6.37844154141217e-05, 'epoch': 1.252}
{'loss': 1.1669, 'grad_norm': 1.6050314903259277, 'learning_rate': 6.318754473153221e-05, 'epoch': 1.256}
{'loss': 1.2457, 'grad_norm': 3.4674713611602783, 'learning_rate': 6.259218726479427e-05, 'epoch': 1.26}
{'loss': 1.0737, 'grad_norm': 0.6866969466209412, 'learning_rate': 6.199836748672153e-05, 'epoch': 1.264}
{'loss': 1.1988, 'grad_norm': 1.2081127166748047, 'learning_rate': 6.140610980691921e-05, 'epoch': 1.268}
{'loss': 1.3538, 'grad_norm': 0.8213911056518555, 'learning_rate': 6.081543857078076e-05, 'epoch': 1.272}
{'loss': 1.1732, 'grad_norm': 0.7381390929222107, 'learning_rate': 6.022637805848723e-05, 'epoch': 1.276}
{'loss': 1.0812, 'grad_norm': 2.2166683673858643, 'learning_rate': 5.9638952484009105e-05, 'epoch': 1.28}
{'loss': 1.2744, 'grad_norm': 2.6767067909240723, 'learning_rate': 5.9053185994110974e-05, 'epoch': 1.284}
{'loss': 1.2584, 'grad_norm': 1.2022624015808105, 'learning_rate': 5.84691026673589e-05, 'epoch': 1.288}
{'loss': 1.3012, 'grad_norm': 1.24746572971344, 'learning_rate': 5.7886726513130784e-05, 'epoch': 1.292}
{'loss': 1.3923, 'grad_norm': 2.0228545665740967, 'learning_rate': 5.730608147062926e-05, 'epoch': 1.296}
{'loss': 1.5083, 'grad_norm': 0.9072334170341492, 'learning_rate': 5.672719140789786e-05, 'epoch': 1.3}
{'loss': 1.2461, 'grad_norm': 5.430138111114502, 'learning_rate': 5.6150080120839734e-05, 'epoch': 1.304}
{'loss': 1.5298, 'grad_norm': 1.7691644430160522, 'learning_rate': 5.5574771332239406e-05, 'epoch': 1.308}
{'loss': 1.1882, 'grad_norm': 2.263882875442505, 'learning_rate': 5.5001288690787886e-05, 'epoch': 1.312}
{'loss': 1.1222, 'grad_norm': 0.9530985951423645, 'learning_rate': 5.442965577011039e-05, 'epoch': 1.316}
{'loss': 1.2907, 'grad_norm': 2.130737066268921, 'learning_rate': 5.385989606779737e-05, 'epoch': 1.32}
{'loss': 1.3759, 'grad_norm': 0.7279485464096069, 'learning_rate': 5.32920330044386e-05, 'epoch': 1.324}
{'loss': 1.2643, 'grad_norm': 1.3793609142303467, 'learning_rate': 5.272608992266039e-05, 'epoch': 1.328}
{'loss': 1.7058, 'grad_norm': 3.7444915771484375, 'learning_rate': 5.2162090086166215e-05, 'epoch': 1.332}
{'loss': 1.298, 'grad_norm': 1.653182864189148, 'learning_rate': 5.160005667878033e-05, 'epoch': 1.336}
{'loss': 1.3065, 'grad_norm': 1.39066481590271, 'learning_rate': 5.1040012803494795e-05, 'epoch': 1.34}
{'loss': 1.312, 'grad_norm': 3.046872138977051, 'learning_rate': 5.048198148151968e-05, 'epoch': 1.3439999999999999}
{'loss': 0.9999, 'grad_norm': 1.6226998567581177, 'learning_rate': 4.992598565133709e-05, 'epoch': 1.3479999999999999}
{'loss': 1.2588, 'grad_norm': 0.5565070509910583, 'learning_rate': 4.9372048167757876e-05, 'epoch': 1.3519999999999999}
{'loss': 1.4745, 'grad_norm': 11.271613121032715, 'learning_rate': 4.882019180098236e-05, 'epoch': 1.3559999999999999}
{'loss': 1.1382, 'grad_norm': 0.8430469632148743, 'learning_rate': 4.8270439235664344e-05, 'epoch': 1.3599999999999999}
{'loss': 1.0195, 'grad_norm': 1.7277368307113647, 'learning_rate': 4.772281306997848e-05, 'epoch': 1.3639999999999999}
{'loss': 1.0699, 'grad_norm': 1.005626916885376, 'learning_rate': 4.717733581469157e-05, 'epoch': 1.3679999999999999}
{'loss': 1.1893, 'grad_norm': 2.2432761192321777, 'learning_rate': 4.66340298922371e-05, 'epoch': 1.3719999999999999}
{'loss': 1.2393, 'grad_norm': 1.5641885995864868, 'learning_rate': 4.6092917635793576e-05, 'epoch': 1.376}
{'loss': 1.0592, 'grad_norm': 1.6494944095611572, 'learning_rate': 4.555402128836642e-05, 'epoch': 1.38}
{'loss': 1.3295, 'grad_norm': 2.059889793395996, 'learning_rate': 4.501736300187378e-05, 'epoch': 1.384}
{'loss': 1.2758, 'grad_norm': 1.845723032951355, 'learning_rate': 4.4482964836235875e-05, 'epoch': 1.388}
{'loss': 1.357, 'grad_norm': 1.9018101692199707, 'learning_rate': 4.3950848758468196e-05, 'epoch': 1.392}
{'loss': 1.1942, 'grad_norm': 1.2979731559753418, 'learning_rate': 4.3421036641778556e-05, 'epoch': 1.396}
{'loss': 1.355, 'grad_norm': 0.8117163181304932, 'learning_rate': 4.2893550264667915e-05, 'epoch': 1.4}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.5052, 'grad_norm': 2.544800281524658, 'learning_rate': 4.236841131003524e-05, 'epoch': 1.404}
{'loss': 1.1919, 'grad_norm': 0.6567398905754089, 'learning_rate': 4.184564136428611e-05, 'epoch': 1.408}
{'loss': 1.1993, 'grad_norm': 0.6932273507118225, 'learning_rate': 4.132526191644549e-05, 'epoch': 1.412}
{'loss': 1.1856, 'grad_norm': 5.500480651855469, 'learning_rate': 4.0807294357274216e-05, 'epoch': 1.416}
{'loss': 1.438, 'grad_norm': 0.8666473031044006, 'learning_rate': 4.029175997838996e-05, 'epoch': 1.42}
{'loss': 1.5654, 'grad_norm': 2.0457143783569336, 'learning_rate': 3.9778679971391785e-05, 'epoch': 1.424}
{'loss': 1.1025, 'grad_norm': 0.6601086258888245, 'learning_rate': 3.926807542698922e-05, 'epoch': 1.428}
{'loss': 1.2262, 'grad_norm': 2.352203130722046, 'learning_rate': 3.875996733413522e-05, 'epoch': 1.432}
{'loss': 1.265, 'grad_norm': 1.5481921434402466, 'learning_rate': 3.8254376579163255e-05, 'epoch': 1.436}
{'loss': 1.1357, 'grad_norm': 1.0264194011688232, 'learning_rate': 3.775132394492906e-05, 'epoch': 1.44}
{'loss': 1.2564, 'grad_norm': 3.124826431274414, 'learning_rate': 3.725083010995611e-05, 'epoch': 1.444}
{'loss': 1.2434, 'grad_norm': 3.068493127822876, 'learning_rate': 3.675291564758565e-05, 'epoch': 1.448}
{'loss': 1.2962, 'grad_norm': 1.1981743574142456, 'learning_rate': 3.630701491346623e-05, 'epoch': 1.452}
{'loss': 1.278, 'grad_norm': 0.9224603772163391, 'learning_rate': 3.581405755817867e-05, 'epoch': 1.456}
{'loss': 1.1236, 'grad_norm': 0.9845507144927979, 'learning_rate': 3.5323738635585754e-05, 'epoch': 1.46}
{'loss': 1.3037, 'grad_norm': 1.4830487966537476, 'learning_rate': 3.483607830077794e-05, 'epoch': 1.464}
{'loss': 1.1348, 'grad_norm': 1.4393447637557983, 'learning_rate': 3.4351096599561585e-05, 'epoch': 1.468}
{'loss': 1.3109, 'grad_norm': 1.3325958251953125, 'learning_rate': 3.386881346763483e-05, 'epoch': 1.472}
{'loss': 1.256, 'grad_norm': 1.4253904819488525, 'learning_rate': 3.3389248729768276e-05, 'epoch': 1.476}
{'loss': 1.4235, 'grad_norm': 1.4725372791290283, 'learning_rate': 3.291242209898997e-05, 'epoch': 1.48}
{'loss': 1.3144, 'grad_norm': 1.9992882013320923, 'learning_rate': 3.243835317577514e-05, 'epoch': 1.484}
{'loss': 1.3142, 'grad_norm': 0.7951242923736572, 'learning_rate': 3.196706144724034e-05, 'epoch': 1.488}
{'loss': 1.2619, 'grad_norm': 3.1409542560577393, 'learning_rate': 3.149856628634267e-05, 'epoch': 1.492}
{'loss': 1.4949, 'grad_norm': 2.1219077110290527, 'learning_rate': 3.103288695108319e-05, 'epoch': 1.496}
{'loss': 1.4837, 'grad_norm': 0.9007052183151245, 'learning_rate': 3.057004258371541e-05, 'epoch': 1.5}
{'loss': 1.407, 'grad_norm': 0.7557085752487183, 'learning_rate': 3.011005220995854e-05, 'epoch': 1.504}
{'loss': 1.3641, 'grad_norm': 0.6710970997810364, 'learning_rate': 2.9652934738215087e-05, 'epoch': 1.508}
{'loss': 1.1247, 'grad_norm': 1.626600742340088, 'learning_rate': 2.919870895879393e-05, 'epoch': 1.512}
{'loss': 1.1544, 'grad_norm': 3.0314924716949463, 'learning_rate': 2.874739354313779e-05, 'epoch': 1.516}
{'loss': 1.4462, 'grad_norm': 0.8392887115478516, 'learning_rate': 2.8299007043055725e-05, 'epoch': 1.52}
{'loss': 1.2949, 'grad_norm': 0.9334829449653625, 'learning_rate': 2.7853567889960564e-05, 'epoch': 1.524}
{'loss': 1.5003, 'grad_norm': 1.0488301515579224, 'learning_rate': 2.741109439411117e-05, 'epoch': 1.528}
{'loss': 1.0637, 'grad_norm': 2.6593596935272217, 'learning_rate': 2.6971604743859935e-05, 'epoch': 1.532}
{'loss': 1.5242, 'grad_norm': 2.9485127925872803, 'learning_rate': 2.6535117004905018e-05, 'epoch': 1.536}
{'loss': 1.3045, 'grad_norm': 0.5610828995704651, 'learning_rate': 2.61016491195478e-05, 'epoch': 1.54}
{'loss': 1.3165, 'grad_norm': 1.5601493120193481, 'learning_rate': 2.5671218905955207e-05, 'epoch': 1.544}
{'loss': 1.4452, 'grad_norm': 1.5958011150360107, 'learning_rate': 2.5243844057427435e-05, 'epoch': 1.548}
{'loss': 1.15, 'grad_norm': 2.2071595191955566, 'learning_rate': 2.4819542141670648e-05, 'epoch': 1.552}
{'loss': 1.1028, 'grad_norm': 1.0908464193344116, 'learning_rate': 2.4398330600074713e-05, 'epoch': 1.556}
{'loss': 1.1176, 'grad_norm': 0.8827077150344849, 'learning_rate': 2.398022674699635e-05, 'epoch': 1.56}
{'loss': 1.2095, 'grad_norm': 0.8975583910942078, 'learning_rate': 2.3565247769047292e-05, 'epoch': 1.564}
{'loss': 1.4593, 'grad_norm': 1.8655325174331665, 'learning_rate': 2.3153410724388013e-05, 'epoch': 1.568}
{'loss': 1.269, 'grad_norm': 2.191416025161743, 'learning_rate': 2.2744732542026348e-05, 'epoch': 1.572}
{'loss': 1.3976, 'grad_norm': 1.0751683712005615, 'learning_rate': 2.2339230021121725e-05, 'epoch': 1.576}
{'loss': 1.6765, 'grad_norm': 5.112256050109863, 'learning_rate': 2.1936919830294467e-05, 'epoch': 1.58}
{'loss': 1.3058, 'grad_norm': 3.0219483375549316, 'learning_rate': 2.153781850694082e-05, 'epoch': 1.584}
{'loss': 1.1989, 'grad_norm': 0.5735954642295837, 'learning_rate': 2.1141942456552998e-05, 'epoch': 1.588}
{'loss': 1.3289, 'grad_norm': 3.9816439151763916, 'learning_rate': 2.0749307952044918e-05, 'epoch': 1.592}
{'loss': 1.296, 'grad_norm': 1.1907767057418823, 'learning_rate': 2.0359931133083187e-05, 'epoch': 1.596}
{'loss': 1.1522, 'grad_norm': 0.8836289644241333, 'learning_rate': 1.9973828005423757e-05, 'epoch': 1.6}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.2426, 'grad_norm': 1.3064653873443604, 'learning_rate': 1.959101444025395e-05, 'epoch': 1.604}
{'loss': 1.2727, 'grad_norm': 2.7586724758148193, 'learning_rate': 1.921150617354006e-05, 'epoch': 1.608}
{'loss': 1.452, 'grad_norm': 1.1080148220062256, 'learning_rate': 1.8835318805380507e-05, 'epoch': 1.612}
{'loss': 1.3786, 'grad_norm': 1.0947850942611694, 'learning_rate': 1.8462467799364526e-05, 'epoch': 1.616}
{'loss': 1.2592, 'grad_norm': 2.0185420513153076, 'learning_rate': 1.8092968481936634e-05, 'epoch': 1.62}
{'loss': 1.3539, 'grad_norm': 1.4310377836227417, 'learning_rate': 1.772683604176656e-05, 'epoch': 1.624}
{'loss': 1.314, 'grad_norm': 5.2268195152282715, 'learning_rate': 1.7364085529124864e-05, 'epoch': 1.6280000000000001}
{'loss': 1.1943, 'grad_norm': 1.5280544757843018, 'learning_rate': 1.7004731855264354e-05, 'epoch': 1.6320000000000001}
{'loss': 1.4665, 'grad_norm': 4.474483013153076, 'learning_rate': 1.6648789791807018e-05, 'epoch': 1.6360000000000001}
{'loss': 1.1091, 'grad_norm': 1.230301856994629, 'learning_rate': 1.6296273970136977e-05, 'epoch': 1.6400000000000001}
{'loss': 1.4883, 'grad_norm': 1.9440995454788208, 'learning_rate': 1.5947198880798953e-05, 'epoch': 1.6440000000000001}
{'loss': 1.0545, 'grad_norm': 4.70466947555542, 'learning_rate': 1.5601578872902634e-05, 'epoch': 1.6480000000000001}
{'loss': 1.3381, 'grad_norm': 0.6586902141571045, 'learning_rate': 1.525942815353284e-05, 'epoch': 1.6520000000000001}
{'loss': 0.9666, 'grad_norm': 2.4789748191833496, 'learning_rate': 1.4920760787165545e-05, 'epoch': 1.6560000000000001}
{'loss': 1.4263, 'grad_norm': 2.134690999984741, 'learning_rate': 1.4585590695089701e-05, 'epoch': 1.6600000000000001}
{'loss': 1.1569, 'grad_norm': 3.0671825408935547, 'learning_rate': 1.4253931654835018e-05, 'epoch': 1.6640000000000001}
{'loss': 1.3841, 'grad_norm': 3.4060544967651367, 'learning_rate': 1.3925797299605647e-05, 'epoch': 1.6680000000000001}
{'loss': 1.5013, 'grad_norm': 2.5917115211486816, 'learning_rate': 1.3601201117719676e-05, 'epoch': 1.6720000000000002}
{'loss': 1.4402, 'grad_norm': 7.282817840576172, 'learning_rate': 1.3280156452054804e-05, 'epoch': 1.6760000000000002}
{'loss': 1.498, 'grad_norm': 5.3692522048950195, 'learning_rate': 1.2962676499499793e-05, 'epoch': 1.6800000000000002}
{'loss': 1.2068, 'grad_norm': 0.5932410955429077, 'learning_rate': 1.2648774310412038e-05, 'epoch': 1.6840000000000002}
{'loss': 1.2134, 'grad_norm': 1.618551254272461, 'learning_rate': 1.2338462788081006e-05, 'epoch': 1.688}
{'loss': 1.195, 'grad_norm': 1.317176342010498, 'learning_rate': 1.2031754688197994e-05, 'epoch': 1.692}
{'loss': 1.2446, 'grad_norm': 1.8996350765228271, 'learning_rate': 1.1728662618331698e-05, 'epoch': 1.696}
{'loss': 1.3435, 'grad_norm': 1.4385831356048584, 'learning_rate': 1.1429199037409987e-05, 'epoch': 1.7}
{'loss': 1.2434, 'grad_norm': 0.7144060134887695, 'learning_rate': 1.1133376255207751e-05, 'epoch': 1.704}
{'loss': 1.1168, 'grad_norm': 0.6822806000709534, 'learning_rate': 1.08412064318409e-05, 'epoch': 1.708}
{'loss': 1.5503, 'grad_norm': 2.531416416168213, 'learning_rate': 1.0552701577266532e-05, 'epoch': 1.712}
{'loss': 1.2382, 'grad_norm': 1.4844268560409546, 'learning_rate': 1.0267873550789208e-05, 'epoch': 1.716}
{'loss': 1.4586, 'grad_norm': 2.5253801345825195, 'learning_rate': 9.986734060573544e-06, 'epoch': 1.72}
{'loss': 1.1279, 'grad_norm': 1.3339935541152954, 'learning_rate': 9.709294663162771e-06, 'epoch': 1.724}
{'loss': 1.2094, 'grad_norm': 0.8036783337593079, 'learning_rate': 9.43556676300389e-06, 'epoch': 1.728}
{'loss': 1.3781, 'grad_norm': 0.9961788058280945, 'learning_rate': 9.165561611978767e-06, 'epoch': 1.732}
{'loss': 1.0644, 'grad_norm': 1.6110302209854126, 'learning_rate': 8.89929030894161e-06, 'epoch': 1.736}
{'loss': 0.9935, 'grad_norm': 0.6981638669967651, 'learning_rate': 8.636763799262793e-06, 'epoch': 1.74}
{'loss': 1.2807, 'grad_norm': 1.2562626600265503, 'learning_rate': 8.377992874378848e-06, 'epoch': 1.744}
{'loss': 1.406, 'grad_norm': 1.2129998207092285, 'learning_rate': 8.122988171348989e-06, 'epoch': 1.748}
{'loss': 1.3066, 'grad_norm': 1.5875440835952759, 'learning_rate': 7.871760172417763e-06, 'epoch': 1.752}
{'loss': 1.2516, 'grad_norm': 1.2498646974563599, 'learning_rate': 7.624319204584207e-06, 'epoch': 1.756}
{'loss': 1.1751, 'grad_norm': 1.9271297454833984, 'learning_rate': 7.380675439177364e-06, 'epoch': 1.76}
{'loss': 1.2061, 'grad_norm': 1.6404473781585693, 'learning_rate': 7.140838891438129e-06, 'epoch': 1.764}
{'loss': 1.1706, 'grad_norm': 1.033085823059082, 'learning_rate': 6.904819420107611e-06, 'epoch': 1.768}
{'loss': 1.2553, 'grad_norm': 0.7338146567344666, 'learning_rate': 6.672626727021847e-06, 'epoch': 1.772}
{'loss': 1.2211, 'grad_norm': 0.8123170137405396, 'learning_rate': 6.444270356713034e-06, 'epoch': 1.776}
{'loss': 1.4738, 'grad_norm': 1.9350513219833374, 'learning_rate': 6.219759696017113e-06, 'epoch': 1.78}
{'loss': 1.3764, 'grad_norm': 11.092470169067383, 'learning_rate': 5.999103973688003e-06, 'epoch': 1.784}
{'loss': 1.1436, 'grad_norm': 1.0213615894317627, 'learning_rate': 5.7823122600182185e-06, 'epoch': 1.788}
{'loss': 1.3687, 'grad_norm': 0.8807603120803833, 'learning_rate': 5.569393466465978e-06, 'epoch': 1.792}
{'loss': 1.3174, 'grad_norm': 1.154183268547058, 'learning_rate': 5.3603563452889525e-06, 'epoch': 1.796}
{'loss': 1.0644, 'grad_norm': 1.8064628839492798, 'learning_rate': 5.1552094891844315e-06, 'epoch': 1.8}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 1.155, 'grad_norm': 1.2487380504608154, 'learning_rate': 4.953961330936174e-06, 'epoch': 1.804}
{'loss': 1.5939, 'grad_norm': 2.335767984390259, 'learning_rate': 4.756620143067725e-06, 'epoch': 1.808}
{'loss': 1.1855, 'grad_norm': 3.841205358505249, 'learning_rate': 4.563194037502405e-06, 'epoch': 1.812}
{'loss': 1.1697, 'grad_norm': 0.8582811951637268, 'learning_rate': 4.373690965229793e-06, 'epoch': 1.8159999999999998}
{'loss': 1.3137, 'grad_norm': 1.6760307550430298, 'learning_rate': 4.188118715978962e-06, 'epoch': 1.8199999999999998}
{'loss': 1.4736, 'grad_norm': 1.5956134796142578, 'learning_rate': 4.006484917898245e-06, 'epoch': 1.8239999999999998}
{'loss': 1.1843, 'grad_norm': 0.9935858249664307, 'learning_rate': 3.828797037241672e-06, 'epoch': 1.8279999999999998}
{'loss': 1.1105, 'grad_norm': 0.5668760538101196, 'learning_rate': 3.6550623780620795e-06, 'epoch': 1.8319999999999999}
{'loss': 1.1629, 'grad_norm': 1.1534804105758667, 'learning_rate': 3.485288081910798e-06, 'epoch': 1.8359999999999999}
{'loss': 1.151, 'grad_norm': 0.8634810447692871, 'learning_rate': 3.3194811275441996e-06, 'epoch': 1.8399999999999999}
{'loss': 1.3712, 'grad_norm': 2.5164880752563477, 'learning_rate': 3.1576483306367535e-06, 'epoch': 1.8439999999999999}
{'loss': 1.1839, 'grad_norm': 1.8192561864852905, 'learning_rate': 2.9997963435008934e-06, 'epoch': 1.8479999999999999}
{'loss': 1.2708, 'grad_norm': 1.731781005859375, 'learning_rate': 2.8459316548135383e-06, 'epoch': 1.8519999999999999}
{'loss': 1.1549, 'grad_norm': 0.7660986185073853, 'learning_rate': 2.6960605893493807e-06, 'epoch': 1.8559999999999999}
{'loss': 1.4534, 'grad_norm': 1.0327856540679932, 'learning_rate': 2.5501893077209403e-06, 'epoch': 1.8599999999999999}
{'loss': 1.243, 'grad_norm': 2.1951992511749268, 'learning_rate': 2.4083238061252567e-06, 'epoch': 1.8639999999999999}
{'loss': 1.2142, 'grad_norm': 1.4770416021347046, 'learning_rate': 2.270469916097462e-06, 'epoch': 1.8679999999999999}
{'loss': 1.4099, 'grad_norm': 3.6371963024139404, 'learning_rate': 2.1366333042710185e-06, 'epoch': 1.8719999999999999}
{'loss': 1.224, 'grad_norm': 1.915037989616394, 'learning_rate': 2.0068194721448497e-06, 'epoch': 1.876}
{'loss': 1.3213, 'grad_norm': 2.249847888946533, 'learning_rate': 1.8810337558571423e-06, 'epoch': 1.88}
{'loss': 1.1875, 'grad_norm': 3.224130868911743, 'learning_rate': 1.7592813259660334e-06, 'epoch': 1.884}
{'loss': 1.4307, 'grad_norm': 1.6962268352508545, 'learning_rate': 1.6415671872370364e-06, 'epoch': 1.888}
{'loss': 1.2061, 'grad_norm': 3.6463115215301514, 'learning_rate': 1.5278961784373268e-06, 'epoch': 1.892}
{'loss': 1.2843, 'grad_norm': 0.7468088865280151, 'learning_rate': 1.4182729721368804e-06, 'epoch': 1.896}
{'loss': 1.4065, 'grad_norm': 2.148273468017578, 'learning_rate': 1.3127020745163365e-06, 'epoch': 1.9}
{'loss': 1.1673, 'grad_norm': 2.67543888092041, 'learning_rate': 1.2111878251818033e-06, 'epoch': 1.904}
{'loss': 1.3811, 'grad_norm': 1.4302254915237427, 'learning_rate': 1.1137343969864544e-06, 'epoch': 1.908}
{'loss': 1.4557, 'grad_norm': 1.8887947797775269, 'learning_rate': 1.0203457958590346e-06, 'epoch': 1.912}
{'loss': 1.1852, 'grad_norm': 0.5889832377433777, 'learning_rate': 9.310258606391675e-07, 'epoch': 1.916}
{'loss': 1.3973, 'grad_norm': 1.8011465072631836, 'learning_rate': 8.457782629195388e-07, 'epoch': 1.92}
{'loss': 1.1657, 'grad_norm': 0.871187686920166, 'learning_rate': 7.646065068949937e-07, 'epoch': 1.924}
{'loss': 1.0903, 'grad_norm': 0.4963257610797882, 'learning_rate': 6.875139292184973e-07, 'epoch': 1.928}
{'loss': 1.5599, 'grad_norm': 2.8217527866363525, 'learning_rate': 6.145036988639663e-07, 'epoch': 1.932}
{'loss': 1.0669, 'grad_norm': 0.6909997463226318, 'learning_rate': 5.455788169960063e-07, 'epoch': 1.936}
{'loss': 1.1831, 'grad_norm': 1.8070051670074463, 'learning_rate': 4.807421168465664e-07, 'epoch': 1.94}
{'loss': 1.1624, 'grad_norm': 2.6833863258361816, 'learning_rate': 4.1999626359842115e-07, 'epoch': 1.944}
{'loss': 1.4788, 'grad_norm': 9.040894508361816, 'learning_rate': 3.633437542756912e-07, 'epoch': 1.948}
{'loss': 1.1934, 'grad_norm': 2.1801681518554688, 'learning_rate': 3.10786917641126e-07, 'epoch': 1.952}
{'loss': 1.1339, 'grad_norm': 2.6353931427001953, 'learning_rate': 2.6232791410043534e-07, 'epoch': 1.956}
{'loss': 1.4291, 'grad_norm': 0.8826903700828552, 'learning_rate': 2.1796873561344967e-07, 'epoch': 1.96}
{'loss': 1.195, 'grad_norm': 3.451798439025879, 'learning_rate': 1.7771120561226318e-07, 'epoch': 1.964}
{'loss': 1.5938, 'grad_norm': 3.0235755443573, 'learning_rate': 1.4155697892624943e-07, 'epoch': 1.968}
{'loss': 1.3449, 'grad_norm': 0.9726009368896484, 'learning_rate': 1.0950754171407119e-07, 'epoch': 1.972}
{'loss': 1.3175, 'grad_norm': 1.6177197694778442, 'learning_rate': 8.156421140254056e-08, 'epoch': 1.976}
{'loss': 1.2536, 'grad_norm': 1.580028772354126, 'learning_rate': 5.7728136632539955e-08, 'epoch': 1.98}
{'loss': 1.4044, 'grad_norm': 1.3195347785949707, 'learning_rate': 3.80002972117377e-08, 'epoch': 1.984}
{'loss': 1.2074, 'grad_norm': 2.2004735469818115, 'learning_rate': 2.2381504074342473e-08, 'epoch': 1.988}
{'loss': 1.4154, 'grad_norm': 4.324026584625244, 'learning_rate': 1.0872399247763287e-08, 'epoch': 1.992}
{'loss': 1.5212, 'grad_norm': 3.7198355197906494, 'learning_rate': 3.4734558262305983e-09, 'epoch': 1.996}
{'loss': 1.3504, 'grad_norm': 3.3831958770751953, 'learning_rate': 1.8497795132299545e-10, 'epoch': 2.0}
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 11337.2332, 'train_samples_per_second': 1.764, 'train_steps_per_second': 0.441, 'train_loss': 1.3868691574096679, 'epoch': 2.0}
Saving the last checkpoint of the model
/home/ubuntu/.cache/pypoetry/virtualenvs/openpowerlifting-KFLyni6a-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Training Done! 💥
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm ▁▂▁▂▁▂▂▆▂▁▁▂▁▂▃█▁▁▁▁▁▁▂▂▂▂▁▂▁▁▃▃▂▄▁▁▁▂▁▁
wandb: train/learning_rate ▄███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:          train/loss █▅▅▃▃▃▄▃▂▂▁▂▂▃▂▃▃▃▁▁▁▂▂▂▃▁▂▂▁▂▃▂▂▃▂▂▁▃▁▂
wandb: 
wandb: Run summary:
wandb:               total_flos 1.054472740253614e+17
wandb:              train/epoch 2.0
wandb:        train/global_step 5000
wandb:          train/grad_norm 3.3832
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.3504
wandb:               train_loss 1.38687
wandb:            train_runtime 11337.2332
wandb: train_samples_per_second 1.764
wandb:   train_steps_per_second 0.441
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/ubuntu/verb-workspace/openpowerlifting/wandb/offline-run-20240618_144504-vreyvyz7
wandb: Find logs at: ./wandb/offline-run-20240618_144504-vreyvyz7/logs


(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) ✗ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    finetune_starcoder2/.DS_Store
        modified:   finetune_starcoder2/checkpoint-1000/README.md
        modified:   finetune_starcoder2/checkpoint-1000/adapter_config.json
        modified:   finetune_starcoder2/checkpoint-1000/adapter_model.safetensors
        modified:   finetune_starcoder2/checkpoint-1000/optimizer.pt
        modified:   finetune_starcoder2/checkpoint-1000/rng_state.pth
        modified:   finetune_starcoder2/checkpoint-1000/scheduler.pt
        modified:   finetune_starcoder2/checkpoint-1000/tokenizer.json
        modified:   finetune_starcoder2/checkpoint-1000/trainer_state.json
        modified:   finetune_starcoder2/checkpoint-1000/training_args.bin
        modified:   finetune_starcoder2/checkpoint-500/README.md
        modified:   finetune_starcoder2/checkpoint-500/adapter_config.json
        modified:   finetune_starcoder2/checkpoint-500/adapter_model.safetensors
        modified:   finetune_starcoder2/checkpoint-500/optimizer.pt
        modified:   finetune_starcoder2/checkpoint-500/rng_state.pth
        modified:   finetune_starcoder2/checkpoint-500/scheduler.pt
        modified:   finetune_starcoder2/checkpoint-500/tokenizer.json
        modified:   finetune_starcoder2/checkpoint-500/trainer_state.json
        modified:   finetune_starcoder2/checkpoint-500/training_args.bin
        modified:   finetune_starcoder2/final_checkpoint/README.md
        modified:   finetune_starcoder2/final_checkpoint/adapter_config.json
        modified:   finetune_starcoder2/final_checkpoint/adapter_model.safetensors
        modified:   finetune_starcoder2/final_checkpoint/tokenizer.json
        modified:   finetune_starcoder2/final_checkpoint/training_args.bin
        deleted:    finetune_starcoder2/runs/Jun18_10-58-27_68c53361b108/events.out.tfevents.1718708485.68c53361b108.1095.0
        deleted:    inference.ipynb
        modified:   wandb/offline-run-20240618_144504-vreyvyz7/logs/debug-internal.log
        modified:   wandb/offline-run-20240618_144504-vreyvyz7/logs/debug.log
        modified:   wandb/offline-run-20240618_144504-vreyvyz7/run-vreyvyz7.wandb

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        finetune_starcoder2/checkpoint-1500/
        finetune_starcoder2/checkpoint-2000/
        finetune_starcoder2/checkpoint-2500/
        finetune_starcoder2/checkpoint-3000/
        finetune_starcoder2/checkpoint-3500/
        finetune_starcoder2/checkpoint-4000/
        finetune_starcoder2/checkpoint-4500/
        finetune_starcoder2/checkpoint-5000/
        training.log
        wandb/offline-run-20240618_144504-vreyvyz7/files/wandb-summary.json

no changes added to commit (use "git add" and/or "git commit -a")

(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) ✗ git add .
(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) ✗ git commit -m 'fine tune starcoder 2 training checkpoints'
[main d5e88f4] fine tune starcoder 2 training checkpoints
 135 files changed, 1206374 insertions(+), 1070 deletions(-)
 delete mode 100644 finetune_starcoder2/.DS_Store
 rewrite finetune_starcoder2/checkpoint-1000/trainer_state.json (62%)
 create mode 100644 finetune_starcoder2/checkpoint-1500/README.md
 create mode 100644 finetune_starcoder2/checkpoint-1500/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-1500/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-1500/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-1500/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-1500/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-1500/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-1500/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-1500/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-1500/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-1500/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-1500/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-1500/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/README.md
 create mode 100644 finetune_starcoder2/checkpoint-2000/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-2000/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-2000/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-2000/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-2000/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-2000/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-2000/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-2000/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/README.md
 create mode 100644 finetune_starcoder2/checkpoint-2500/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-2500/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-2500/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-2500/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-2500/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-2500/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-2500/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-2500/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/README.md
 create mode 100644 finetune_starcoder2/checkpoint-3000/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-3000/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-3000/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-3000/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-3000/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-3000/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-3000/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-3000/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/README.md
 create mode 100644 finetune_starcoder2/checkpoint-3500/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-3500/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-3500/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-3500/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-3500/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-3500/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-3500/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-3500/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/README.md
 create mode 100644 finetune_starcoder2/checkpoint-4000/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-4000/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-4000/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-4000/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-4000/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-4000/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-4000/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-4000/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/README.md
 create mode 100644 finetune_starcoder2/checkpoint-4500/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-4500/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-4500/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-4500/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-4500/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-4500/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-4500/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-4500/vocab.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/README.md
 create mode 100644 finetune_starcoder2/checkpoint-5000/adapter_config.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/adapter_model.safetensors
 create mode 100644 finetune_starcoder2/checkpoint-5000/merges.txt
 create mode 100644 finetune_starcoder2/checkpoint-5000/optimizer.pt
 create mode 100644 finetune_starcoder2/checkpoint-5000/rng_state.pth
 create mode 100644 finetune_starcoder2/checkpoint-5000/scheduler.pt
 create mode 100644 finetune_starcoder2/checkpoint-5000/special_tokens_map.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/tokenizer.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/tokenizer_config.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/trainer_state.json
 create mode 100644 finetune_starcoder2/checkpoint-5000/training_args.bin
 create mode 100644 finetune_starcoder2/checkpoint-5000/vocab.json
 delete mode 100644 finetune_starcoder2/runs/Jun18_10-58-27_68c53361b108/events.out.tfevents.1718708485.68c53361b108.1095.0
 delete mode 100644 inference.ipynb
 create mode 100644 training.log
 create mode 100644 wandb/offline-run-20240618_144504-vreyvyz7/files/wandb-summary.json
(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) git push origin main
Enumerating objects: 97, done.
Counting objects: 100% (97/97), done.
Delta compression using up to 16 threads
Compressing objects: 100% (75/75), done.
Writing objects: 100% (76/76), 239.49 MiB | 14.63 MiB/s, done.
Total 76 (delta 45), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (45/45), completed with 9 local objects.
To https://github.com/tonymoszuti/openpowerlifting.git
   14979fc..d5e88f4  main -> main
(openpowerlifting-py3.10) ➜  openpowerlifting git:(main) git status
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean